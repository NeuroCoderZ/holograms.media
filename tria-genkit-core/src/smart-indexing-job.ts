// tria-genkit-core/src/smart-indexing-job.ts
import * as path from 'path';
import * as fs from 'fs/promises';
import { genkit } from 'genkit';
import { googleAI, textEmbedding004 } from '@genkit-ai/googleai';
import postgres from 'postgres';
import pgvector from 'pgvector';
import { RecursiveCharacterTextSplitter } from 'langchain/text_splitter';
import { glob } from 'glob';
import crypto from 'crypto';

const ai = genkit({
  plugins: [googleAI()],
});

const sql = postgres(process.env.DATABASE_URL!);

interface ProcessingReport {
  processedFiles: string[];
  newEmbeddings: number;
  updatedEmbeddings: number;
  skippedFiles: string[];
  errors: string[];
  startTime: string;
  endTime: string;
  totalTime: string;
}

async function runSmartIndexing() {
  console.log('üß† –ó–∞–ø—É—Å–∫ —É–º–Ω–æ–π –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤...');
  
  const report: ProcessingReport = {
    processedFiles: [],
    newEmbeddings: 0,
    updatedEmbeddings: 0,
    skippedFiles: [],
    errors: [],
    startTime: new Date().toISOString(),
    endTime: '',
    totalTime: ''
  };

  const startTime = Date.now();
  
  try {
    // –°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü
    await initializeDatabase();
    
    // –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    const filesToProcess = await getFilesToProcess();
    console.log(`üìÅ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: ${filesToProcess.length}`);
    
    const batchSize = parseInt(process.env.BATCH_SIZE || '10');
    
    // –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –±–∞—Ç—á–∞–º–∏
    for (let i = 0; i < filesToProcess.length; i += batchSize) {
      const batch = filesToProcess.slice(i, i + batchSize);
      console.log(`üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ batch ${Math.floor(i/batchSize) + 1}/${Math.ceil(filesToProcess.length/batchSize)}`);
      
      await Promise.all(batch.map(async (filePath) => {
        try {
          const result = await processFile(filePath);
          report.processedFiles.push(filePath);
          report.newEmbeddings += result.newChunks;
          report.updatedEmbeddings += result.updatedChunks;
        } catch (error) {
          console.error(`‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ ${filePath}:`, error);
          report.errors.push(`${filePath}: ${error.message}`);
        }
      }));
      
      // –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
      await new Promise(resolve => setTimeout(resolve, 2000));
    }
    
    report.endTime = new Date().toISOString();
    report.totalTime = `${Math.round((Date.now() - startTime) / 1000)}s`;
    
    // –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
    await saveReport(report);
    
    console.log('üéâ –£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!');
    console.log(`üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: ${report.newEmbeddings} –Ω–æ–≤—ã—Ö, ${report.updatedEmbeddings} –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤`);
    
  } catch (error) {
    console.error('‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞:', error);
    process.exit(1);
  }
}

async function initializeDatabase() {
  console.log('üóÑÔ∏è –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...');
  
  await sql`CREATE EXTENSION IF NOT EXISTS vector`;
  
  // –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
  await sql`
    CREATE TABLE IF NOT EXISTS holograms_media_embeddings (
      id SERIAL PRIMARY KEY,
      file_path TEXT NOT NULL,
      chunk_hash TEXT NOT NULL,
      content TEXT NOT NULL,
      embedding VECTOR(768),
      metadata JSONB,
      created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
      updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
      UNIQUE(file_path, chunk_hash)
    );
  `;
  
  // –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
  await sql`
    CREATE TABLE IF NOT EXISTS file_metadata (
      file_path TEXT PRIMARY KEY,
      file_hash TEXT NOT NULL,
      last_modified TIMESTAMP NOT NULL,
      chunk_count INTEGER DEFAULT 0,
      processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    );
  `;
  
  // –ò–Ω–¥–µ–∫—Å—ã –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
  await sql`CREATE INDEX IF NOT EXISTS idx_embeddings_file_path ON holograms_media_embeddings(file_path);`;
  await sql`CREATE INDEX IF NOT EXISTS idx_embeddings_vector ON holograms_media_embeddings USING ivfflat (embedding vector_cosine_ops);`;
}

async function getFilesToProcess(): Promise<string[]> {
  const changedFiles = process.env.CHANGED_FILES?.split(',').filter(f => f.trim()) || [];
  const forceRebuild = process.env.FORCE_REBUILD === 'true';
  
  let filesToProcess: string[] = [];
  
  if (changedFiles.length > 0 && !forceRebuild) {
    // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    filesToProcess = changedFiles.filter(f => f.match(/\.(md|js|py|txt|json)$/));
    console.log('üìù –†–µ–∂–∏–º –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è');
  } else {
    // –°–∫–∞–Ω–∏—Ä—É–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
    const patterns = [
      'data/**/*.{md,txt,json}',
      'frontend/**/*.js',
      'backend/**/*.py',
      'tria-genkit-core/src/**/*.ts'
    ];
    
    for (const pattern of patterns) {
      const files = await glob(pattern, { 
        cwd: process.cwd(),
        ignore: ['node_modules/**', '.git/**', 'dist/**', 'build/**']
      });
      filesToProcess.push(...files);
    }
    
    console.log('üîÑ –†–µ–∂–∏–º –ø–æ–ª–Ω–æ–≥–æ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è');
  }
  
  console.log(`üìã –§–∞–π–ª–æ–≤ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ: ${filesToProcess.length}`);
  return filesToProcess;
}

async function processFile(filePath: string) {
  const absolutePath = path.resolve(filePath);
  
  // –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
  const stats = await fs.stat(absolutePath);
  const content = await fs.readFile(absolutePath, 'utf-8');
  
  // –í—ã—á–∏—Å–ª—è–µ–º —Ö–µ—à —Ñ–∞–π–ª–∞
  const fileHash = crypto.createHash('md5').update(content).digest('hex');
  
  // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ñ–∞–π–ª
  const existingFile = await sql`
    SELECT file_hash, processed_at FROM file_metadata 
    WHERE file_path = ${filePath}
  `;
  
  const forceRebuild = process.env.FORCE_REBUILD === 'true';
  
  if (existingFile.length > 0 && existingFile[0].file_hash === fileHash && !forceRebuild) {
    console.log(`‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º ${filePath} (–Ω–µ –∏–∑–º–µ–Ω–∏–ª—Å—è)`);
    return { newChunks: 0, updatedChunks: 0 };
  }
  
  console.log(`üîÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º ${filePath}...`);
  
  // –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —ç—Ç–æ–≥–æ —Ñ–∞–π–ª–∞
  await sql`DELETE FROM holograms_media_embeddings WHERE file_path = ${filePath}`;
  
  // –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫–∏
  const splitter = new RecursiveCharacterTextSplitter({
    chunkSize: 1000,
    chunkOverlap: 200,
  });
  
  const chunks = await splitter.splitText(content);
  console.log(`üìÑ ${filePath}: —Å–æ–∑–¥–∞–Ω–æ ${chunks.length} —á–∞–Ω–∫–æ–≤`);
  
  let processedChunks = 0;
  
  // –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫–∏
  for (const [index, chunk] of chunks.entries()) {
    if (chunk.trim().length === 0) continue;
    
    const chunkHash = crypto.createHash('md5')
      .update(`${filePath}:${index}:${chunk}`)
      .digest('hex');
    
    // –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥
    const embeddingResponse = await ai.embed({
      embedder: textEmbedding004,
      content: chunk,
    });
    
    const embedding = Array.isArray(embeddingResponse) 
      ? embeddingResponse[0].embedding 
      : embeddingResponse.embedding;
    
    const formattedEmbedding = pgvector.toSql(embedding);
    
    // –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    const metadata = {
      file_path: filePath,
      chunk_index: index,
      chunk_size: chunk.length,
      file_type: path.extname(filePath),
      processed_at: new Date().toISOString()
    };
    
    // –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –±–∞–∑—É
    await sql`
      INSERT INTO holograms_media_embeddings 
      (file_path, chunk_hash, content, embedding, metadata)
      VALUES (${filePath}, ${chunkHash}, ${chunk}, ${formattedEmbedding}, ${metadata})
      ON CONFLICT (file_path, chunk_hash) 
      DO UPDATE SET 
        content = EXCLUDED.content,
        embedding = EXCLUDED.embedding,
        metadata = EXCLUDED.metadata,
        updated_at = CURRENT_TIMESTAMP
    `;
    
    processedChunks++;
  }
  
  // –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞
  await sql`
    INSERT INTO file_metadata (file_path, file_hash, last_modified, chunk_count)
    VALUES (${filePath}, ${fileHash}, ${stats.mtime}, ${processedChunks})
    ON CONFLICT (file_path)
    DO UPDATE SET 
      file_hash = EXCLUDED.file_hash,
      last_modified = EXCLUDED.last_modified,
      chunk_count = EXCLUDED.chunk_count,
      processed_at = CURRENT_TIMESTAMP
  `;
  
  console.log(`‚úÖ ${filePath}: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ ${processedChunks} —á–∞–Ω–∫–æ–≤`);
  return { newChunks: processedChunks, updatedChunks: 0 };
}

async function saveReport(report: ProcessingReport) {
  const reportPath = path.resolve('embedding-report.json');
  const logPath = path.resolve('processing-log.txt');
  
  await fs.writeFile(reportPath, JSON.stringify(report, null, 2));
  
  const logContent = [
    `=== EMBEDDING PIPELINE REPORT ===`,
    `Start Time: ${report.startTime}`,
    `End Time: ${report.endTime}`,
    `Total Time: ${report.totalTime}`,
    ``,
    `üìä STATISTICS:`,
    `- Processed Files: ${report.processedFiles.length}`,
    `- New Embeddings: ${report.newEmbeddings}`,
    `- Updated Embeddings: ${report.updatedEmbeddings}`,
    `- Skipped Files: ${report.skippedFiles.length}`,
    `- Errors: ${report.errors.length}`,
    ``,
    `üìÅ PROCESSED FILES:`,
    ...report.processedFiles.map(f => `‚úÖ ${f}`),
    ``,
    `‚ö†Ô∏è ERRORS:`,
    ...report.errors.map(e => `‚ùå ${e}`),
  ].join('\n');
  
  await fs.writeFile(logPath, logContent);
  
  console.log(`üìã –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: ${reportPath}`);
}

// –ó–∞–ø—É—Å–∫
runSmartIndexing()
  .then(() => {
    console.log('‚úÖ –£–º–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ');
    process.exit(0);
  })
  .catch(error => {
    console.error('‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞:', error);
    process.exit(1);
  });
